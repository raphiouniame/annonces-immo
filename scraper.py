import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import json
from database import init_database, save_annonce
import random

# Initialiser la base de donn√©es
init_database()

def generate_fake_contact():
    """G√©n√©rer des contacts fictifs pour les annonces de d√©monstration"""
    noms = ['Kouassi Jean', 'Adjoua Marie', 'Koffi Paul', 'Akissi Sandra', 'Yao Michel', 
            'Ama Fatou', 'Ouattara Ali', 'Diabat√© Sekou', 'N\'Guessan Eric', 'Bamba Salif']
    
    nom = random.choice(noms)
    
    # G√©n√©rer un num√©ro ivoirien fictif
    prefixes = ['07', '05', '01']  # Pr√©fixes t√©l√©phoniques ivoiriens
    numero = f"+225 {random.choice(prefixes)} {random.randint(10,99)} {random.randint(10,99)} {random.randint(10,99)}"
    
    # Email fictif
    email = f"{nom.lower().replace(' ', '.')}@gmail.com"
    
    # WhatsApp (m√™me num√©ro que le t√©l√©phone)
    whatsapp = numero
    
    return {
        'nom': nom,
        'telephone': numero,
        'email': email,
        'whatsapp': whatsapp
    }

def scrape_tonkro():
    """Scraping basique de Tonkro.ci (√† adapter selon le site r√©el)"""
    annonces = []
    
    try:
        # Exemple d'URL de recherche (√† adapter)
        url = "https://tonkro.ci/search?category=immobilier&location=abidjan"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Exemple de parsing (√† adapter selon la structure r√©elle du site)
        # annonces_elements = soup.find_all('div', class_='annonce-item')
        
        # Pour l'exemple, cr√©ons quelques annonces fictives avec contacts
        quartiers_abidjan = ['Plateau', 'Cocody', 'Treichville', 'Marcory', 'Yopougon']
        types = ['vente', 'location']
        
        for i in range(3):  # 3 annonces par ex√©cution
            contact = generate_fake_contact()
            annonce = {
                'id': int(time.time() * 1000000) + i,  # ID unique
                'titre': f"Appartement {random.randint(2,4)} pi√®ces - {random.choice(quartiers_abidjan)}",
                'description': "Bel appartement bien situ√© dans un quartier calme",
                'prix': str(random.randint(50000, 500000000)),  # En FCFA
                'type': random.choice(types),
                'quartier': random.choice(quartiers_abidjan),
                'surface': f"{random.randint(50, 200)} m¬≤",
                'chambres': random.randint(1, 5),
                'date_publication': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Tonkro.ci',
                'url': f"https://tonkro.ci/annonce/{int(time.time() * 1000000) + i}",
                'contact_nom': contact['nom'],
                'contact_telephone': contact['telephone'],
                'contact_email': contact['email'],
                'contact_whatsapp': contact['whatsapp']
            }
            annonces.append(annonce)
            
    except Exception as e:
        print(f"Erreur scraping Tonkro: {e}")
    
    return annonces

def scrape_jumia_deal():
    """Scraping basique de Jumia Deal CI"""
    annonces = []
    
    try:
        # Exemple d'URL (√† adapter)
        url = "https://deals.jumia.ci/immobilier-abidjan"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        # Parsing √† impl√©menter selon la structure r√©elle
        
        # Pour l'exemple, cr√©ons quelques annonces
        quartiers_abidjan = ['Plateau', 'Cocody', 'Treichville', 'Marcory', 'Rivera']
        types = ['vente', 'location']
        
        for i in range(2):
            contact = generate_fake_contact()
            annonce = {
                'id': int(time.time() * 1000000) + 100 + i,
                'titre': f"Villa {random.randint(3,6)} chambres - {random.choice(quartiers_abidjan)}",
                'description': "Grande villa avec jardin dans quartier r√©sidentiel",
                'prix': str(random.randint(100000000, 800000000)),  # En FCFA
                'type': random.choice(types),
                'quartier': random.choice(quartiers_abidjan),
                'surface': f"{random.randint(150, 500)} m¬≤",
                'chambres': random.randint(3, 6),
                'date_publication': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Jumia Deal CI',
                'url': f"https://deals.jumia.ci/annonce/{int(time.time() * 1000000) + 100 + i}",
                'contact_nom': contact['nom'],
                'contact_telephone': contact['telephone'],
                'contact_email': contact['email'],
                'contact_whatsapp': contact['whatsapp']
            }
            annonces.append(annonce)
            
    except Exception as e:
        print(f"Erreur scraping Jumia Deal: {e}")
    
    return annonces

def scrape_afribaba():
    """Scraping basique d'Afribaba CI"""
    annonces = []
    
    try:
        # Pour l'exemple, cr√©ons quelques annonces
        quartiers_abidjan = ['Bingerville', 'Anyama', 'Koumassi', 'Port-Bouet']
        
        for i in range(2):
            contact = generate_fake_contact()
            annonce = {
                'id': int(time.time() * 1000000) + 200 + i,
                'titre': f"Terrain √† vendre - {random.choice(quartiers_abidjan)}",
                'description': "Terrain plat dans zone en d√©veloppement",
                'prix': str(random.randint(20000000, 200000000)),  # En FCFA
                'type': 'vente',
                'quartier': random.choice(quartiers_abidjan),
                'surface': f"{random.randint(200, 1000)} m¬≤",
                'chambres': 0,
                'date_publication': datetime.now().strftime('%Y-%m-%d'),
                'source': 'Afribaba CI',
                'url': f"https://ci.afribaba.com/annonce/{int(time.time() * 1000000) + 200 + i}",
                'contact_nom': contact['nom'],
                'contact_telephone': contact['telephone'],
                'contact_email': contact['email'],
                'contact_whatsapp': contact['whatsapp']
            }
            annonces.append(annonce)
            
    except Exception as e:
        print(f"Erreur scraping Afribaba: {e}")
    
    return annonces

def fetch_daily_ads():
    """Fonction principale pour r√©cup√©rer les annonces du jour"""
    print(f"üîÑ R√©cup√©ration des annonces du {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    all_annonces = []
    
    # R√©cup√©rer les annonces de diff√©rentes sources
    all_annonces.extend(scrape_tonkro())
    all_annonces.extend(scrape_jumia_deal())
    all_annonces.extend(scrape_afribaba())
    
    # Sauvegarder les annonces
    saved_count = 0
    for annonce in all_annonces:
        if save_annonce(annonce):
            saved_count += 1
    
    print(f"‚úÖ {saved_count}/{len(all_annonces)} annonces sauvegard√©es")
    return all_annonces

def main():
    """Fonction principale du scraper"""
    print("üöÄ D√©marrage du scraper d'annonces immobili√®res d'Abidjan")
    
    # Ex√©cuter une fois au d√©marrage
    fetch_daily_ads()
    
    # Boucle d'ex√©cution
    while True:
        try:
            print(f"‚è∞ Prochaine ex√©cution dans 24 heures...")
            time.sleep(24 * 60 * 60)  # Attendre 24 heures
            annonces = fetch_daily_ads()
        except KeyboardInterrupt:
            print("üõë Scraper arr√™t√© par l'utilisateur")
            break
        except Exception as e:
            print(f"‚ùå Erreur: {e}")
            print("‚è∞ Nouvelle tentative dans 1 heure...")
            time.sleep(60 * 60)  # Attendre 1 heure en cas d'erreur

if __name__ == "__main__":
    main()